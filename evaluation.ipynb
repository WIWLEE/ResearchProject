{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 13:19:13.632040: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-22 13:19:14.231420: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/undergrad/miniconda3/envs/tf-2.12.0/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-22 13:19:20.089974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46672 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-04-22 13:19:20.091238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46672 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:23:00.0, compute capability: 8.6\n",
      "2024-04-22 13:19:20.092140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 46672 MB memory:  -> device: 2, name: NVIDIA RTX A6000, pci bus id: 0000:41:00.0, compute capability: 8.6\n",
      "2024-04-22 13:19:20.093020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 46672 MB memory:  -> device: 3, name: NVIDIA RTX A6000, pci bus id: 0000:61:00.0, compute capability: 8.6\n",
      "2024-04-22 13:19:20.093875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 46672 MB memory:  -> device: 4, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2024-04-22 13:19:20.094721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 46672 MB memory:  -> device: 5, name: NVIDIA RTX A6000, pci bus id: 0000:a1:00.0, compute capability: 8.6\n",
      "2024-04-22 13:19:20.095560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 46672 MB memory:  -> device: 6, name: NVIDIA RTX A6000, pci bus id: 0000:c1:00.0, compute capability: 8.6\n",
      "2024-04-22 13:19:20.096393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 46672 MB memory:  -> device: 7, name: NVIDIA RTX A6000, pci bus id: 0000:e1:00.0, compute capability: 8.6\n",
      "2024-04-22 13:19:21.596978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'lm_head.weight', 'transformer.h.2.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.10.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFGPT2LMHeadModel #tensorflow GPT2 LM\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "#mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'prompt.txt'\n",
    "f = open(filename, 'r')\n",
    "lines = f.readlines()\n",
    "print(lines)\n",
    "print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    print(f\"prompt : {line}\")\n",
    "\n",
    "    input_text = line\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
    "    #term = 3\n",
    "\n",
    "    last_index = input_ids.shape[-1]\n",
    "\n",
    "    #with mirrored_strategy.scope():\n",
    "    while input_ids.shape[-1] < 100:\n",
    "        #print(\"================================\")\n",
    "        #print(f\"문장의 토큰 개수 : {input_ids.shape[-1]}\")\n",
    "        #set 나눠서 green set 로짓을 촉진하기\n",
    "        gamma = 1.0\n",
    "        delta = 0.0\n",
    "        Logits = model(input_ids).logits\n",
    "        #print(f\"original Logits : {Logits}\")\n",
    "        \n",
    "        #seed 생성\n",
    "        seed = input_ids[-1] % Logits.shape[-1]\n",
    "        #print(f\"seed : {seed[-1]}\")\n",
    "        np.random.seed(seed[-1])\n",
    "\n",
    "        #random green list 설정\n",
    "        indices = np.arange(Logits.shape[-1]) #0~51999\n",
    "        green_list_size = int(Logits.shape[-1] * gamma)  #25600\n",
    "        green_list = np.random.choice(indices, green_list_size, replace=False)\n",
    "        #print(f\"green list : {green_list} {green_list.shape}\") #25600개의 green list\n",
    "\n",
    "        #--여기부터 probability 구하기\n",
    "        # green logit과 red logit의 값을 담을 리스트 생성\n",
    "        green_logits = tf.zeros(green_list_size)\n",
    "        red_logits = tf.zeros(Logits.shape[-1] - green_list_size)\n",
    "\n",
    "        #그린 리스트에 속한 idx 값의 Logit에는 델타 추가!!!!!!\n",
    "        Logits_num = np.zeros((0,last_index-1,Logits.shape[-1]))\n",
    "        Logits_num = Logits.numpy()\n",
    "        for i in green_list :\n",
    "            Logits_num[0,last_index-1,i] = Logits_num[0,last_index-1,i]+delta\n",
    "        Logits = tf.convert_to_tensor(Logits_num, dtype=tf.float32)\n",
    "        #print(f\"updated Logits : {Logits}\")\n",
    "        \n",
    "        # softmax 계산을 위한 분모\n",
    "        denominator = tf.reduce_sum(tf.exp(Logits), axis=-1)  \n",
    "        #print(f\"denominator : {denominator}\") #25600개의 green list\n",
    "\n",
    "        # Logits 텐서에 softmax 적용하여 확률 계산 (병렬 계산)\n",
    "        softmax_tensor = Logits.numpy()\n",
    "        softmax_tensor[0, last_index-1, :] = np.exp(softmax_tensor[0, last_index-1, :])\n",
    "        softmax_tensor[0, last_index-1, :] /= denominator[0,last_index-1]\n",
    "        #print(softmax_tensor.shape)\n",
    "        #print(f\"Probabilities : {softmax_tensor}\")\n",
    "\n",
    "        # 데이터 타입 확인 및 조정\n",
    "        add_token_id = tf.argmax(softmax_tensor[0][last_index-1])\n",
    "        #print(f\"updated Logits Max value : {add_token_id}\")\n",
    "\n",
    "\n",
    "        # 텐서 연결\n",
    "        add_token_id = tf.reshape(add_token_id, [1,1])\n",
    "        add_token_id = tf.cast(add_token_id, input_ids.dtype)\n",
    "        input_ids = tf.concat([input_ids, add_token_id], axis=-1)\n",
    "        next_token = tokenizer.decode(input_ids.numpy()[0,:])\n",
    "        #print(f\"next token : {next_token}\")\n",
    "        \n",
    "        last_index = input_ids.shape[-1]\n",
    "        #term = 3\n",
    "    final_token = tokenizer.decode(input_ids.numpy()[0,:])\n",
    "    print(final_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity 계산 이렇게 한대요\n",
    "# 확률에 로그 적용 (로그 확률 계산)\n",
    "log_probabilities = tf.math.log(probabilities)\n",
    "\n",
    "# 실제 데이터에서 각 단어의 인덱스가 필요 (예시를 위해 무작위 인덱스 생성)\n",
    "# 실제 사용 시에는 이 부분을 실제 테스트 데이터셋의 단어 인덱스로 대체해야 함\n",
    "true_word_indices = tf.random.uniform([10], minval=0, maxval=5000, dtype=tf.int32)\n",
    "\n",
    "# 실제 단어의 로그 확률을 선택\n",
    "true_log_probabilities = tf.gather(log_probabilities, true_word_indices, axis=1, batch_dims=1)\n",
    "\n",
    "# 평균 로그 확률 계산\n",
    "mean_log_prob = tf.reduce_mean(true_log_probabilities)\n",
    "\n",
    "# Perplexity 계산\n",
    "perplexity = tf.exp(-mean_log_prob)\n",
    "print(\"Calculated Perplexity:\", perplexity.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-2.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
