{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "from transformers import AutoModelWithLMHead, PreTrainedTokenizerFast\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3.9871e-08, 3.6668e-07, 8.1882e-08,  ..., 1.5475e-08,\n",
      "          4.5798e-08, 1.4672e-08],\n",
      "         [1.2173e-04, 5.3592e-04, 3.9241e-05,  ..., 2.5251e-04,\n",
      "          4.4066e-04, 1.4473e-04],\n",
      "         [1.1599e-05, 2.8659e-04, 2.8215e-03,  ..., 3.4654e-05,\n",
      "          4.1377e-05, 3.4650e-04],\n",
      "         ...,\n",
      "         [5.4043e-05, 2.0140e-03, 6.9852e-04,  ..., 2.0787e-04,\n",
      "          1.8033e-03, 7.4471e-04],\n",
      "         [9.0218e-05, 4.0489e-04, 5.8880e-04,  ..., 8.5230e-04,\n",
      "          1.9085e-03, 2.1960e-04],\n",
      "         [1.1889e-04, 2.2530e-04, 2.7579e-03,  ..., 1.2470e-04,\n",
      "          4.9531e-03, 6.3066e-04]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([1, 15, 768])\n",
      "max prob vector index : 136\n",
      ":-)8<\n",
      "max prob vector index : 281\n",
      "😯\n",
      "max prob vector index : 281\n",
      "😯\n",
      "max prob vector index : 172\n",
      ":8)\n",
      "max prob vector index : 423\n",
      "Q\n",
      "max prob vector index : 283\n",
      "😗\n",
      "max prob vector index : 471\n",
      "±\n",
      "max prob vector index : 137\n",
      ":-O\n",
      "max prob vector index : 763\n",
      "➀\n",
      "max prob vector index : 601\n",
      "ᄋ\n",
      "max prob vector index : 477\n",
      "Ä\n",
      "max prob vector index : 114\n",
      "(:-(\n",
      "max prob vector index : 477\n",
      "Ä\n",
      "max prob vector index : 423\n",
      "Q\n",
      "max prob vector index : 763\n",
      "➀\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저와 모델 불러오기\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",bos_token='</s>', eos_token='</s>', unk_token='<unk>',pad_token='<pad>', mask_token='<mask>')\n",
    "model = GPT2Model.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "\n",
    "# 프롬프트 정의\n",
    "prompt = \"한국어 자연어 처리 모델에 대한 특정 질문입니다\" + \"이건 두번째 문장\"\n",
    "\n",
    "# 토크나이징 및 모델 입력 형식으로 변환\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=True)\n",
    "\n",
    "# 모델에 입력하여 출력 받기\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 로짓 텐서\n",
    "logits_tensor = outputs.last_hidden_state\n",
    "\n",
    "# softmax 함수를 적용하여 확률 분포로 변환\n",
    "probabilities = F.softmax(logits_tensor, dim=-1)\n",
    "\n",
    "# 결과 확인\n",
    "print(probabilities)\n",
    "print(probabilities.size())\n",
    "\n",
    "for i in range(probabilities.size()[1]) :\n",
    "    max_prob_vector_index = torch.argmax(probabilities[0][i])\n",
    "    print(f\"max prob vector index : {max_prob_vector_index}\")\n",
    "    #decode to string\n",
    "    var = tokenizer.decode(max_prob_vector_index)\n",
    "    print(var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[33245, 10114, 12748, 11357, 23879, 39306,  9684,  7884, 10211, 15177,\n",
      "         26421,   387, 17339,  7889,  9908, 15768,  6903, 15386,  8146, 12923,\n",
      "          9228, 18651, 42600,  9564, 17764,  9033,  9199, 14441,  7335,  8704,\n",
      "         12557, 32030,  9510, 18595,  9025, 10571, 25741, 10599, 13229,  9508,\n",
      "          7965,  8425, 33102,  9122, 21240,  9801, 32106, 13579, 12442, 13235,\n",
      "         19430,  8022, 12972,  9566, 11178,  9554, 24873,  7198,  9391, 12486,\n",
      "          8711,  9346,  7071, 36736,  9693, 12006,  9038, 10279, 36122,  9960,\n",
      "          8405, 10826, 18988, 25998,  9292,  7671,  9465,  7489,  9277, 10137,\n",
      "          9677,  9248,  9912, 12834, 11488, 13417,  7407,  8428,  8137,  9430,\n",
      "         14222, 11356, 10061,  9885, 19265,  9377, 20305,  7991,  9178,  9648,\n",
      "          9133, 10021, 10138, 30315, 21833,  9362,  9301,  9685, 11584,  9447,\n",
      "         42129, 10124,  7532, 17932, 47123, 37544,  9355, 15632,  9124, 10536,\n",
      "         13530, 12204,  9184, 36152,  9673,  9788,  9029, 11764,  9337, 10212,\n",
      "         14226, 46503,  9676, 26992,  8135,  9443, 11994,  9825, 13023, 18520,\n",
      "             8, 12199,  7187, 15179, 21734,  9563, 19367, 13386,  7426,  9751,\n",
      "         11380,  8367,  7756,  9686,  6905,   389,  9018,  7056,  8033, 37022,\n",
      "          9620, 15034, 10095, 14932, 32009, 21361,  6920,  7556,  9690,  9136,\n",
      "          9432, 28096,  9513, 13363, 30601, 11965,  9180,  9172, 10453, 25002,\n",
      "         12803,  7109, 10148,   681,  9030,  9628, 27286,  9141, 14702,  9167,\n",
      "         28282, 23029,  9532,  6963,  7244, 13054, 27013,   384,  9555,  7669]])\n",
      "근육이 커지기 위해서는 무엇보다 규칙적인 생활습관이 중요하다.\n",
      "특히, 아침식사는 단백질과 비타민이 풍부한 과일과 채소를 많이 섭취하는 것이 좋다.\n",
      "또한 하루 30분 이상 충분한 수면을 취하는 것도 도움이 된다.\n",
      "아침 식사를 거르지 않고 규칙적으로 운동을 하면 혈액순환에 도움을 줄 뿐만 아니라 신진대사를 촉진해 체내 노폐물을 배출하고 혈압을 낮춰준다.\n",
      "운동은 하루에 10분 정도만 하는 게 좋으며 운동 후에는 반드시 스트레칭을 통해 근육량을 늘리고 유연성을 높여야 한다.\n",
      "운동 후 바로 잠자리에 드는 것은 피해야 하며 특히 아침에 일어나면 몸이 피곤해지기 때문에 무리하게 움직이면 오히려 역효과가 날 수도 있다.\n",
      "운동을 할 때는 몸을 따뜻하게 하고 땀은 잘 흡수하도록 해야 한다.</d> 지난달 30일 오후 서울 종로구 세종로 정부중앙청사 별관. 이낙연 국무총리가 주재하던 국무회의가 열렸다.\n",
      "국무위원들이 모두 참석한 가운데 열린 이날 회의에서는 ‘경제혁신 3개년 계획’ 등 주요 국정 현안에 대한 논의가 이뤄졌다.\n",
      "김동연(사진) 경제부\n"
     ]
    }
   ],
   "source": [
    "text = '근육이 커지기 위해서는'\n",
    "input_ids = tokenizer.encode(text) #encode : text -> vocabulary\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=200,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "\n",
    "print(gen_ids)\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist()) # decode() : tokenizer 와 vocabulary를 이용해서 token id를 string으로 변환\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#여기부터 watermark 한국어 적용 시작!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "/home/undergrad/miniconda3/envs/tf-2.12.0/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1595: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "2024-03-30 00:06:16.419206: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-30 00:06:16.461014: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-30 00:06:17.097194: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "옛날 옛날 어느 마을에 흥부와 놀부 형제가 왁자지껄 떠들어대며 \"우리 집엔 왜 이렇게 많은 사람들이 모여 사는 거야?\" 하고 묻는다.\n",
      "그런데 그 마을 사람들은 모두 다들 자기네 동네에 살고 있는 사람들이라고 한다.\n",
      "이렇게 해서 우리 마을은 '흥부가 살던 곳'이라는 뜻의 '고향'이 되었다.\n",
      "그리고 이 고향은 바로 지금의 서울 종로구 숭인동이다.\n",
      "숭인동은 원래 종로에서 가장 오래된 주택가였다.\n",
      "1970년대까지만 해도 이곳은 재개발로 인해 헐리고 빈집이 많아졌다.\n",
      "하지만 1980년대 들어 다시 활기를 되찾기 시작했다.\n",
      "당시만 하더라도 이곳에는 낡은 건물들이 많이 남아 있었다.\n",
      "그러나 1990년대 들어서부터는 예전의\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, PreTrainedTokenizerFast\n",
    "import torch\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "#import torch\n",
    "#from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>') \n",
    "model = AutoModelWithLMHead.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "\n",
    "text = \"\"\" 옛날 옛날 어느 마을에 흥부와 놀부 형제가 \"\"\"\n",
    "input_ids = tokenizer.encode(text)\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True\n",
    "                        )\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'reduce_sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# 로짓 조정 값\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 워터마킹이 적용된 다음 토큰 생성\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m watermarked_token \u001b[38;5;241m=\u001b[39m \u001b[43mwatermarking_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m워터마크 포함된 예측 토큰 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwatermarked_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mwatermarking_logits\u001b[0;34m(prompt, previous_token, gamma, delta)\u001b[0m\n\u001b[1;32m      8\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 이전 토큰의 해시로 난수 생성기 초기화\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#seed = int(hashlib.sha256(previous_token.encode()).hexdigest(), 16) % 2**32\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#np.random.seed(seed)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_sum\u001b[49m(input_ids) \u001b[38;5;241m%\u001b[39m logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-2.12.0/lib/python3.8/site-packages/torch/__init__.py:1938\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1935\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 1938\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'reduce_sum'"
     ]
    }
   ],
   "source": [
    "\n",
    "def watermarking_logits(prompt, previous_token, gamma, delta):\n",
    "    # 프롬프트와 이전 토큰으로 입력 구성\n",
    "    input_ids = tokenizer.encode(prompt + previous_token, return_tensors='pt')\n",
    "\n",
    "    # 로짓 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "    # 이전 토큰의 해시로 난수 생성기 초기화\n",
    "    #seed = int(hashlib.sha256(previous_token.encode()).hexdigest(), 16) % 2**32\n",
    "    #np.random.seed(seed)\n",
    "    seed = int(torch.reduce_sum(input_ids) % logits.shape[-1])\n",
    "    print(f\"seed : {seed}\")\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 전체 토큰 집합을 그린 리스트와 레드 리스트로 구분\n",
    "    token_indices = np.arange(logits.size(1)) #생성할 수 있는 모든 토큰\n",
    "    print(f\"total tokens list size : {len(token_indices)} \")\n",
    "    green_list_size = int(len(token_indices) * gamma)\n",
    "    print(f\"green token list size : {green_list_size}\")\n",
    "    green_list_indices = np.random.choice(token_indices, green_list_size, replace=False)\n",
    "    print(f\"green token list : {green_list_indices}\")\n",
    "    \n",
    "    # 그린 리스트에 속하는 토큰의 로짓에 δ 더하기 => 생성 촉진\n",
    "    for idx in green_list_indices:\n",
    "        logits[0, idx] += delta\n",
    "\n",
    "    # 수정된 로짓을 기반으로 다음 토큰 예측\n",
    "    next_token_id = torch.argmax(logits, dim=-1) #마찬가지로 마지막 차원, 전체 로짓중에서 가장 큰 값\n",
    "    next_token = tokenizer.decode(next_token_id)\n",
    "    \n",
    "    return next_token\n",
    "\n",
    "# 예시 실행\n",
    "prompt = \"이것은 예시 문장입니다. \"\n",
    "previous_token = \"예시\"\n",
    "gamma = 0.5  # 그린 리스트 비율\n",
    "delta = 0.1  # 로짓 조정 값\n",
    "\n",
    "# 워터마킹이 적용된 다음 토큰 생성\n",
    "watermarked_token = watermarking_logits(prompt, previous_token, gamma, delta)\n",
    "print(f\"워터마크 포함된 예측 토큰 : {watermarked_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded previous_token : 까?\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [12818 32527  1365 ... 13888 45783 42271]\n",
      "next token : \"\n",
      "\n",
      "decoded previous_token : \"\n",
      "\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42159 21111 44876 ... 50794  4268 21424]\n",
      "next token : \"\n",
      "decoded previous_token : \"\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [10692 14837 17766 ... 17030  7821  9039]\n",
      "next token : 흥\n",
      "decoded previous_token : 흥\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [48266 35904  4251 ... 26463 26110  5132]\n",
      "next token : 부\n",
      "decoded previous_token : 부\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [39611  2347 40860 ... 11600 24423  1536]\n",
      "next token : 에게\n",
      "decoded previous_token : 에게\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [49850 37290  1581 ... 45209    90 48108]\n",
      "next token : 무엇을\n",
      "decoded previous_token : 무엇을\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [26783  5268 13710 ... 35357 25920 44496]\n",
      "next token : 했\n",
      "decoded previous_token : 했\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42090 25360 10031 ... 37300 26570 27395]\n",
      "next token : 지\n",
      "decoded previous_token : 지\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [44166 25530 11705 ... 11309 32169 27507]\n",
      "next token : ?\n",
      "decoded previous_token : ?\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [31049 32714 23032 ... 19986 13097 27945]\n",
      "next token : \"\n",
      "\n",
      "decoded previous_token : \"\n",
      "\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42159 21111 44876 ... 50794  4268 21424]\n",
      "next token : \"\n",
      "decoded previous_token : \"\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [10692 14837 17766 ... 17030  7821  9039]\n",
      "next token : 흥\n",
      "decoded previous_token : 흥\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [48266 35904  4251 ... 26463 26110  5132]\n",
      "next token : 부\n",
      "decoded previous_token : 부\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [39611  2347 40860 ... 11600 24423  1536]\n",
      "next token : 에게\n",
      "decoded previous_token : 에게\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [49850 37290  1581 ... 45209    90 48108]\n",
      "next token : 무엇을\n",
      "decoded previous_token : 무엇을\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [26783  5268 13710 ... 35357 25920 44496]\n",
      "next token : 했\n",
      "decoded previous_token : 했\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42090 25360 10031 ... 37300 26570 27395]\n",
      "next token : 지\n",
      "decoded previous_token : 지\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [44166 25530 11705 ... 11309 32169 27507]\n",
      "next token : ?\n",
      "decoded previous_token : ?\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [31049 32714 23032 ... 19986 13097 27945]\n",
      "next token : \"\n",
      "\n",
      "decoded previous_token : \"\n",
      "\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42159 21111 44876 ... 50794  4268 21424]\n",
      "next token : \"\n",
      "decoded previous_token : \"\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [10692 14837 17766 ... 17030  7821  9039]\n",
      "next token : 흥\n",
      "decoded previous_token : 흥\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [48266 35904  4251 ... 26463 26110  5132]\n",
      "next token : 부\n",
      "decoded previous_token : 부\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [39611  2347 40860 ... 11600 24423  1536]\n",
      "next token : 에게\n",
      "decoded previous_token : 에게\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [49850 37290  1581 ... 45209    90 48108]\n",
      "next token : 무\n",
      "decoded previous_token : 무\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [30431 41558 40217 ... 41728 16103 14174]\n",
      "next token : 엇\n",
      "decoded previous_token : 엇\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [28633 21803 50719 ... 18090 10793 30586]\n",
      "next token : 을\n",
      "decoded previous_token : 을\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [25638  1471 12313 ... 11944   847 47478]\n",
      "next token : 했\n",
      "decoded previous_token : 했\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42090 25360 10031 ... 37300 26570 27395]\n",
      "next token : 지\n",
      "decoded previous_token : 지\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [44166 25530 11705 ... 11309 32169 27507]\n",
      "next token : ?\n",
      "decoded previous_token : ?\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [31049 32714 23032 ... 19986 13097 27945]\n",
      "next token : \"\n",
      "\n",
      "decoded previous_token : \"\n",
      "\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42159 21111 44876 ... 50794  4268 21424]\n",
      "next token : \"\n",
      "decoded previous_token : \"\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [10692 14837 17766 ... 17030  7821  9039]\n",
      "next token : 흥\n",
      "decoded previous_token : 흥\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [48266 35904  4251 ... 26463 26110  5132]\n",
      "next token : 부\n",
      "decoded previous_token : 부\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [39611  2347 40860 ... 11600 24423  1536]\n",
      "next token : 에게\n",
      "decoded previous_token : 에게\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [49850 37290  1581 ... 45209    90 48108]\n",
      "next token : 무\n",
      "decoded previous_token : 무\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [30431 41558 40217 ... 41728 16103 14174]\n",
      "next token : 엇\n",
      "decoded previous_token : 엇\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [28633 21803 50719 ... 18090 10793 30586]\n",
      "next token : 을\n",
      "decoded previous_token : 을\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [25638  1471 12313 ... 11944   847 47478]\n",
      "next token : 했\n",
      "decoded previous_token : 했\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42090 25360 10031 ... 37300 26570 27395]\n",
      "next token : 지\n",
      "decoded previous_token : 지\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [44166 25530 11705 ... 11309 32169 27507]\n",
      "next token : ?\n",
      "decoded previous_token : ?\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [31049 32714 23032 ... 19986 13097 27945]\n",
      "next token : \"\n",
      "\n",
      "decoded previous_token : \"\n",
      "\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42159 21111 44876 ... 50794  4268 21424]\n",
      "next token : \"\n",
      "decoded previous_token : \"\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [10692 14837 17766 ... 17030  7821  9039]\n",
      "next token : 흥\n",
      "decoded previous_token : 흥\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [48266 35904  4251 ... 26463 26110  5132]\n",
      "next token : 부\n",
      "decoded previous_token : 부\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [39611  2347 40860 ... 11600 24423  1536]\n",
      "next token : 에게\n",
      "decoded previous_token : 에게\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [49850 37290  1581 ... 45209    90 48108]\n",
      "next token : 무\n",
      "decoded previous_token : 무\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [30431 41558 40217 ... 41728 16103 14174]\n",
      "next token : 엇\n",
      "decoded previous_token : 엇\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [28633 21803 50719 ... 18090 10793 30586]\n",
      "next token : 을\n",
      "decoded previous_token : 을\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [25638  1471 12313 ... 11944   847 47478]\n",
      "next token : 했\n",
      "decoded previous_token : 했\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42090 25360 10031 ... 37300 26570 27395]\n",
      "next token : 지\n",
      "decoded previous_token : 지\n",
      "==================================================\n",
      "옛날 옛날 어느 마을에 흥부와 놀부 형제가 살고 있었다. 흥부는 놀부에게 무엇을 했을까?\"\n",
      "\"흥부에게무엇을했지?\"\n",
      "\"흥부에게무엇을했지?\"\n",
      "\"흥부에게무엇을했지?\"\n",
      "\"흥부에게무엇을했지?\"\n",
      "\"흥부에게무엇을했지\n"
     ]
    }
   ],
   "source": [
    "def generate_with_watermarking(model, tokenizer, prompt, gamma, delta, max_length):\n",
    "    generated_text = prompt\n",
    "    previous_token = tokenizer.encode(prompt, add_special_tokens=False)[-1:] #토큰화, 가장 마지막 토큰 선택\n",
    "    previous_token = tokenizer.decode(previous_token)\n",
    "    print(f\"decoded previous_token : {previous_token}\")\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        next_token = watermarking_logits(generated_text, previous_token, gamma, delta)\n",
    "        print(f\"next token : {next_token}\")\n",
    "        generated_text += next_token\n",
    "        previous_token = next_token\n",
    "        print(f\"decoded previous_token : {previous_token}\")\n",
    "\n",
    "        print(\"==================================================\")\n",
    "        if next_token == tokenizer.eos_token:\n",
    "            break\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# 사용 예시\n",
    "prompt = \"옛날 옛날 어느 마을에 흥부와 놀부 형제가 살고 있었다. 흥부는 놀부에게 무엇을 했을까?\"\n",
    "max_length = 50  # 생성할 최대 토큰 수\n",
    "gamma = 0.5  # 그린 리스트 비율\n",
    "delta = 0.1  # 로짓 조정 값\n",
    "\n",
    "# 워터마크 포함 텍스트 생성\n",
    "watermarked_text = generate_with_watermarking(model, tokenizer, prompt, gamma, delta, max_length)\n",
    "#collection_generated_text += watermarked_text\n",
    "print(watermarked_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
