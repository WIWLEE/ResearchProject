{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 23:47:13.003078: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-24 23:47:13.591232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/undergrad/miniconda3/envs/tf-2.12.0/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFGPT2LMHeadModel #tensorflow GPT2 LM\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(final_sequence, logit_shape, gamma) :\n",
    "    \n",
    "    input_ids = tokenizer.encode(final_sequence, return_tensors='tf')\n",
    "    input_ids_list = input_ids.numpy().tolist()[0]\n",
    "    prev_tk = input_ids[0]\n",
    "    green_counts = 0\n",
    "    \n",
    "    for id in input_ids_list :\n",
    "        seed = id % logit_shape\n",
    "        #print(f\"seed : {seed[-1]}\")\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        indices = np.arange(logit_shape) #0~51999\n",
    "        green_list_size = int(logit_shape * gamma)  #25600\n",
    "        green_list = np.random.choice(indices, green_list_size, replace=False)\n",
    "       #print(f\"detection greenList : {green_list}\")\n",
    "        \n",
    "        if id in green_list :\n",
    "            green_counts = green_counts + 1\n",
    "        prev_id = id\n",
    "\n",
    "    print(f\"green list 단어 개수 : {green_counts}\")\n",
    "    print(f\"red list 단어 개수 : {len(input_ids_list)-green_counts}\")\n",
    "    green_proportion = green_counts/len(input_ids_list)\n",
    "    red_proportion = 1-green_proportion\n",
    "    print(f\"green list 단어 비율 : {green_proportion}\")\n",
    "    print(f\"red list 단어 비율 : {red_proportion}\")\n",
    "    expected_proportion = 0.5 #green list 를 모를 것이다 -> H0\n",
    "    std_error = np.sqrt(expected_proportion * (1 - expected_proportion) / len(input_ids))\n",
    "    z_score = (green_proportion - expected_proportion) / std_error\n",
    "    p_value = 2 * norm.sf(np.abs(z_score))\n",
    "    \n",
    "    print(f\"Z-score: {z_score}, P-value: {p_value}\")\n",
    "    if p_value < 0.05 :\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "#g=0.7, d=10.0 ->red list 단어 비율 : 0.580000000000000\n",
    "#sentence = \"책 읽기의 즐거움도 함께 느낄 수 있다. 이번 전시는 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 진행된다. 전시에는 책 읽는 즐거움이란 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로\"\n",
    "#g=0.5, d=50.0 -> red list 단어 비율 : 0.5252525252525253\n",
    "#sentence = \"책 읽기의 즐거움도 함께 느낄 것이다.이번 전시는 '책과 세상을 잇는 길'이라는 주제 하에 작가인 김혜숙씨가 참여했다. 이번 전시는 '책과 세상을 잇는 길'이라는 주제 하에 김혜숙씨가 참여한 '책과 세상을 잇는 길'이라는 주제 하에 작가인 김혜숙씨가 참여한 '책과 세상을 잇는 길'이라는 주제 하에 진행됐고 전시는 '책과 세상을 잇는 길'이라는 주제 하에 진행됐고 전시는 '책과 세상을 잇는 길'이라는 주제 하에 진행됐고 전시는\"\n",
    "#g=0.5, d=20.0 -> red list 단어 비율 : 0.53\n",
    "#sentence = \"책 읽기의 즐거움도 함께 느낄 것이다.이번 전시는 '책과 세상을 잇는 길'이라는 주제 하에 작가인 김혜숙씨가 참여했다. 이번 전시는 '책과 세상을 잇는 길'이라는 주제 하에 김혜숙씨가 참여한 '책과 세상을 잇는 길'이라는 주제 하에 작가인 김혜숙씨가 참여한 '책과 세상을 잇는 길'이라는 주제 하에 진행됐고 전시는 '책과 세상을 잇는 길'이라는 주제 하에 진행됐고 전시는 '책과 세상을 잇는 길'이라는 주제 하에 진행됐고 전시는 \"\n",
    "#g=0.5, d=10.0 -> red list 단어 비율 : 0.5252525252525253\n",
    "#sentence = \"책 읽기의 즐거움도 함께 느낄 것이다.이번 전시는 '책과 세상을 잇는 길'이라는 주제 하에 작가인 김혜숙씨가 참여했다. 이번 전시는 '책과 세상을 잇는 길'이라는 주제 하에 김혜숙씨가 참여한 '책과 세상을 잇는 길'이라는 주제 하에 작가인 김혜숙씨가 참여한 '책과 세상을 잇는 길'이라는 주제 하에 진행됐고 전시는 '책과 세상을 잇는 길'이라는 주제 하에 진행됐고 전시는 '책과 세상을 잇는 길'이라는 주제 하에 진행됐고 전시는\"\n",
    "#g=0.5, d=5.0 -> red list 단어 비율 : 0.505050505050505\n",
    "#sentence = \"책 읽기의 즐거움도 함께 느낄 수 있다. 이번 전시는 '책과 나눔'이라는 주제 하에 지난달 27일 개막한 '책과 나눔'전을 통해 처음 선보였으며, 오는 9월까지 계속된다. 특히, 전시 기간 동안 '책과 나눔'전을 통해서는 '책과 나눔'전을 통해서는 '책과 나눔'전을 통해서는 '책과 나눔'전을 통해서는 '책과 나눔'전을 통해서는 '책과 나눔'전을 통해서는 '책과 나눔'전을 통해\"\n",
    "#g=0.5, d=1.0 -> red list 단어 비율 : 0.6\n",
    "#sentence = \"책 읽기의 즐거움도 함께 느낄 수 있다.이번 전시는 ‘책 읽는 즐거움’을 주제로 한 ‘책 읽는 즐거움’ 전시를 비롯해 ‘책 읽는 즐거움’ 전시, ‘책 읽는 즐거움’ 전시, ‘책 읽는 즐거움’ 전시, ‘책 읽는 즐거움’ 전시, ‘책 읽는 즐거움’ 전시, ‘책 읽는 즐거움’ 전시, ‘책 읽는 즐거움’ 전시, ‘책 읽는 즐거움’ 전시, ‘책 읽는 즐거움\"\n",
    "#g=1.0, d=0.0 -> red list 단어 비율 : 0.5700000000000001\n",
    "#sentence = \"책 읽기의 즐거움도 함께 느낄 수 있다. 이번 전시는 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 진행된다. 전시에서는 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는 주제로 '책 읽는 즐거움'이라는\"\n",
    "\n",
    "#g=0.5\n",
    "#sentence = \"아침 일과 시작하기 전에 미리 준비해두면 좋겠습니다. 오늘은 전국이 대체로 맑겠습니다. 하지만 제주도는 구름이 많이 끼겠고요. 강원 영동 지역은 밤 한때 비가 조금 내리겠습니다. 내일 아침 기온 오늘과 비슷하겠습니다. 서울이 4도, 대구 6도로 출발하겠습니다. 낮 기온은 서울이 13도, 대구와 광주 16도로 오늘과 비슷하겠습니다. 바다의 물결은 동해 먼바다에서 최고 2.5미터까지 높게 일겠습니다. 내일 아침 기온 오늘과 비슷하겠습니다.\"\n",
    "\n",
    "#사람\n",
    "#sentence = \"안정된 순방향 전파: 만약 우리가 오프셋 δ를 0으로 초기화하고 스케일 γ를 1로 초기화한다면, 각 출력 활성화는 단위 분산을 갖게 됩니다. 일반적인 네트워크에서는 이는 초기화 과정에서 순방향 전파 중에 분산이 안정적으로 유지됨을 보장합니다. 하지만 잔차 네트워크에서는 분산은 여전히 새로운 변동 요소가 각 레이어에서 입력에 추가되면서 증가해야 합니다. 그러나 잔차 블록마다 분산은 선형적으로 증가할 것입니다. k번째 레이어는 k의 기존\"\n",
    "\n",
    "#사람\n",
    "#sentence = \"이건 사람이 작성한 글입니다. 절대 절대 p-value 가 낮게 나와서도 안되고 z-score가 0 근처여서도 안됩니다. 제발 좀 잘 나와보자 제발제발 할말이 없네 이게 다야\"\n",
    "#detection(sentence, 25600, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the repetition penalty\n",
    "penalty_coefficient = 1.3\n",
    "\n",
    "# Function to apply dynamic repetition penalty to logits\n",
    "def apply_repetition_penalty(logits, token_counts, penalty_coefficient):\n",
    "    penalty_factors = np.ones(logits.shape[-1])\n",
    "\n",
    "    for token_id, count in enumerate(token_counts):\n",
    "        if count > 0:\n",
    "            penalty_factors[token_id] = penalty_coefficient ** count\n",
    "\n",
    "    # Apply the calculated penalties to logits\n",
    "    return logits / penalty_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    print(f\"prompt : {line}\")\n",
    "\n",
    "    input_text = line\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
    "    #term = 3\n",
    "\n",
    "    last_index = input_ids.shape[-1]\n",
    "    nlls = []\n",
    "    token_counts = np.zeros(tokenizer.vocab_size)\n",
    "    with mirrored_strategy.scope():\n",
    "        for _ in tqdm(range(100)):  # 최대 100번 반복\n",
    "            if input_ids.shape[-1] >= 100:\n",
    "                break  # input_ids의 길이가 100 이상이면 반복 중단\n",
    "            \n",
    "            #print(\"================================\")\n",
    "            #print(f\"문장의 토큰 개수 : {input_ids.shape[-1]}\")\n",
    "            #set 나눠서 green set 로짓을 촉진하기\n",
    "            gamma = 0.7\n",
    "            delta = 10.0\n",
    "            Logits = model(input_ids).logits\n",
    "            #print(f\"original Logits : {Logits}\")\n",
    "            \n",
    "            #seed 생성\n",
    "            seed = input_ids[-1] % Logits.shape[-1]\n",
    "            print(f\"seed : {seed[-1]}\")\n",
    "            np.random.seed(seed[-1])\n",
    "\n",
    "            #random green list 설정\n",
    "            indices = np.arange(Logits.shape[-1]) #0~51999\n",
    "            green_list_size = int(Logits.shape[-1] * gamma)  #25600\n",
    "            green_list = np.random.choice(indices, green_list_size, replace=False)\n",
    "            print(f\"green list : {green_list} {green_list.shape}\") #25600개의 green list\n",
    "\n",
    "            #--여기부터 probability 구하기\n",
    "            # green logit과 red logit의 값을 담을 리스트 생성\n",
    "            green_logits = tf.zeros(green_list_size)\n",
    "            red_logits = tf.zeros(Logits.shape[-1] - green_list_size)\n",
    "\n",
    "            #그린 리스트에 속한 idx 값의 Logit에는 델타 추가!!!!!!\n",
    "            Logits_num = np.zeros((0,last_index-1,Logits.shape[-1]))\n",
    "            Logits_num = Logits.numpy()\n",
    "            #Logits_num[0, -1, :] = apply_repetition_penalty(Logits_num[0, -1, :], token_counts, penalty_coefficient)\n",
    "            \n",
    "            for i in green_list :\n",
    "                Logits_num[0,last_index-1,i] = Logits_num[0,last_index-1,i]+delta\n",
    "            Logits = tf.convert_to_tensor(Logits_num, dtype=tf.float32)\n",
    "            #print(f\"updated Logits : {Logits}\")\n",
    "            \n",
    "            # softmax 계산을 위한 분모\n",
    "            denominator = tf.reduce_sum(tf.exp(Logits), axis=-1)  \n",
    "            #print(f\"denominator : {denominator}\") #25600개의 green list\n",
    "\n",
    "            # Logits 텐서에 softmax 적용하여 확률 계산 (병렬 계산), softmax_tensor = probability\n",
    "            softmax_tensor = Logits.numpy()\n",
    "            softmax_tensor[0, last_index-1, :] = np.exp(softmax_tensor[0, last_index-1, :])\n",
    "            softmax_tensor[0, last_index-1, :] /= denominator[0,last_index-1]\n",
    "            #print(softmax_tensor.shape)\n",
    "            #print(f\"Probabilities : {softmax_tensor}\")\n",
    "            #print(softmax_tensor[0, last_index-1, add_token_id])\n",
    "\n",
    "            # 데이터 타입 확인 및 조정\n",
    "            add_token_id = tf.argmax(softmax_tensor[0][last_index-1])\n",
    "            #print(f\"updated Logits Max value : {add_token_id}\")\n",
    "            \n",
    "            # Update token counts\n",
    "            #token_counts[add_token_id] += 1\n",
    "\n",
    "            \n",
    "            # Calculate perplexity\n",
    "            # 참고 https://huggingface.co/docs/transformers/main/ko/perplexity\n",
    "            selected_probability = softmax_tensor[0, last_index-1, add_token_id]\n",
    "            nlls.append(np.log(selected_probability))\n",
    "            \n",
    "            # 텐서 연결\n",
    "            add_token_id = tf.reshape(add_token_id, [1,1])\n",
    "            add_token_id = tf.cast(add_token_id, input_ids.dtype)\n",
    "            input_ids = tf.concat([input_ids, add_token_id], axis=-1)\n",
    "            next_token = tokenizer.decode(input_ids.numpy()[0,:])\n",
    "            print(f\"next token : {next_token}\")\n",
    "            \n",
    "            last_index = input_ids.shape[-1]\n",
    "            #term = 3\n",
    "        final_token = tokenizer.decode(input_ids.numpy()[0,:])\n",
    "        print(final_token)\n",
    "        print(f\"is this sentence written by machine? {detection(final_token, Logits.shape[-1], gamma)}\")\n",
    "\n",
    "        # Calculate perplexity\n",
    "        perplexity = tf.exp(-tf.reduce_mean(nlls))\n",
    "        print(\"Perplexity: \", perplexity.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-2.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
