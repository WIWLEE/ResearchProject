{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "from transformers import AutoModelWithLMHead, PreTrainedTokenizerFast\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3.9871e-08, 3.6668e-07, 8.1882e-08,  ..., 1.5475e-08,\n",
      "          4.5798e-08, 1.4672e-08],\n",
      "         [1.2173e-04, 5.3592e-04, 3.9241e-05,  ..., 2.5251e-04,\n",
      "          4.4066e-04, 1.4473e-04],\n",
      "         [1.1599e-05, 2.8659e-04, 2.8215e-03,  ..., 3.4654e-05,\n",
      "          4.1377e-05, 3.4650e-04],\n",
      "         ...,\n",
      "         [5.4043e-05, 2.0140e-03, 6.9852e-04,  ..., 2.0787e-04,\n",
      "          1.8033e-03, 7.4471e-04],\n",
      "         [9.0218e-05, 4.0489e-04, 5.8880e-04,  ..., 8.5230e-04,\n",
      "          1.9085e-03, 2.1960e-04],\n",
      "         [1.1889e-04, 2.2530e-04, 2.7579e-03,  ..., 1.2470e-04,\n",
      "          4.9531e-03, 6.3066e-04]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([1, 15, 768])\n",
      "max prob vector index : 136\n",
      ":-)8<\n",
      "max prob vector index : 281\n",
      "ğŸ˜¯\n",
      "max prob vector index : 281\n",
      "ğŸ˜¯\n",
      "max prob vector index : 172\n",
      ":8)\n",
      "max prob vector index : 423\n",
      "Q\n",
      "max prob vector index : 283\n",
      "ğŸ˜—\n",
      "max prob vector index : 471\n",
      "Â±\n",
      "max prob vector index : 137\n",
      ":-O\n",
      "max prob vector index : 763\n",
      "â€\n",
      "max prob vector index : 601\n",
      "á„‹\n",
      "max prob vector index : 477\n",
      "Ã„\n",
      "max prob vector index : 114\n",
      "(:-(\n",
      "max prob vector index : 477\n",
      "Ã„\n",
      "max prob vector index : 423\n",
      "Q\n",
      "max prob vector index : 763\n",
      "â€\n"
     ]
    }
   ],
   "source": [
    "# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",bos_token='</s>', eos_token='</s>', unk_token='<unk>',pad_token='<pad>', mask_token='<mask>')\n",
    "model = GPT2Model.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "prompt = \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì— ëŒ€í•œ íŠ¹ì • ì§ˆë¬¸ì…ë‹ˆë‹¤\" + \"ì´ê±´ ë‘ë²ˆì§¸ ë¬¸ì¥\"\n",
    "\n",
    "# í† í¬ë‚˜ì´ì§• ë° ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=True)\n",
    "\n",
    "# ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì¶œë ¥ ë°›ê¸°\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# ë¡œì§“ í…ì„œ\n",
    "logits_tensor = outputs.last_hidden_state\n",
    "\n",
    "# softmax í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜\n",
    "probabilities = F.softmax(logits_tensor, dim=-1)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(probabilities)\n",
    "print(probabilities.size())\n",
    "\n",
    "for i in range(probabilities.size()[1]) :\n",
    "    max_prob_vector_index = torch.argmax(probabilities[0][i])\n",
    "    print(f\"max prob vector index : {max_prob_vector_index}\")\n",
    "    #decode to string\n",
    "    var = tokenizer.decode(max_prob_vector_index)\n",
    "    print(var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[33245, 10114, 12748, 11357, 23879, 39306,  9684,  7884, 10211, 15177,\n",
      "         26421,   387, 17339,  7889,  9908, 15768,  6903, 15386,  8146, 12923,\n",
      "          9228, 18651, 42600,  9564, 17764,  9033,  9199, 14441,  7335,  8704,\n",
      "         12557, 32030,  9510, 18595,  9025, 10571, 25741, 10599, 13229,  9508,\n",
      "          7965,  8425, 33102,  9122, 21240,  9801, 32106, 13579, 12442, 13235,\n",
      "         19430,  8022, 12972,  9566, 11178,  9554, 24873,  7198,  9391, 12486,\n",
      "          8711,  9346,  7071, 36736,  9693, 12006,  9038, 10279, 36122,  9960,\n",
      "          8405, 10826, 18988, 25998,  9292,  7671,  9465,  7489,  9277, 10137,\n",
      "          9677,  9248,  9912, 12834, 11488, 13417,  7407,  8428,  8137,  9430,\n",
      "         14222, 11356, 10061,  9885, 19265,  9377, 20305,  7991,  9178,  9648,\n",
      "          9133, 10021, 10138, 30315, 21833,  9362,  9301,  9685, 11584,  9447,\n",
      "         42129, 10124,  7532, 17932, 47123, 37544,  9355, 15632,  9124, 10536,\n",
      "         13530, 12204,  9184, 36152,  9673,  9788,  9029, 11764,  9337, 10212,\n",
      "         14226, 46503,  9676, 26992,  8135,  9443, 11994,  9825, 13023, 18520,\n",
      "             8, 12199,  7187, 15179, 21734,  9563, 19367, 13386,  7426,  9751,\n",
      "         11380,  8367,  7756,  9686,  6905,   389,  9018,  7056,  8033, 37022,\n",
      "          9620, 15034, 10095, 14932, 32009, 21361,  6920,  7556,  9690,  9136,\n",
      "          9432, 28096,  9513, 13363, 30601, 11965,  9180,  9172, 10453, 25002,\n",
      "         12803,  7109, 10148,   681,  9030,  9628, 27286,  9141, 14702,  9167,\n",
      "         28282, 23029,  9532,  6963,  7244, 13054, 27013,   384,  9555,  7669]])\n",
      "ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” ë¬´ì—‡ë³´ë‹¤ ê·œì¹™ì ì¸ ìƒí™œìŠµê´€ì´ ì¤‘ìš”í•˜ë‹¤.\n",
      "íŠ¹íˆ, ì•„ì¹¨ì‹ì‚¬ëŠ” ë‹¨ë°±ì§ˆê³¼ ë¹„íƒ€ë¯¼ì´ í’ë¶€í•œ ê³¼ì¼ê³¼ ì±„ì†Œë¥¼ ë§ì´ ì„­ì·¨í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\n",
      "ë˜í•œ í•˜ë£¨ 30ë¶„ ì´ìƒ ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ëŠ” ê²ƒë„ ë„ì›€ì´ ëœë‹¤.\n",
      "ì•„ì¹¨ ì‹ì‚¬ë¥¼ ê±°ë¥´ì§€ ì•Šê³  ê·œì¹™ì ìœ¼ë¡œ ìš´ë™ì„ í•˜ë©´ í˜ˆì•¡ìˆœí™˜ì— ë„ì›€ì„ ì¤„ ë¿ë§Œ ì•„ë‹ˆë¼ ì‹ ì§„ëŒ€ì‚¬ë¥¼ ì´‰ì§„í•´ ì²´ë‚´ ë…¸íë¬¼ì„ ë°°ì¶œí•˜ê³  í˜ˆì••ì„ ë‚®ì¶°ì¤€ë‹¤.\n",
      "ìš´ë™ì€ í•˜ë£¨ì— 10ë¶„ ì •ë„ë§Œ í•˜ëŠ” ê²Œ ì¢‹ìœ¼ë©° ìš´ë™ í›„ì—ëŠ” ë°˜ë“œì‹œ ìŠ¤íŠ¸ë ˆì¹­ì„ í†µí•´ ê·¼ìœ¡ëŸ‰ì„ ëŠ˜ë¦¬ê³  ìœ ì—°ì„±ì„ ë†’ì—¬ì•¼ í•œë‹¤.\n",
      "ìš´ë™ í›„ ë°”ë¡œ ì ìë¦¬ì— ë“œëŠ” ê²ƒì€ í”¼í•´ì•¼ í•˜ë©° íŠ¹íˆ ì•„ì¹¨ì— ì¼ì–´ë‚˜ë©´ ëª¸ì´ í”¼ê³¤í•´ì§€ê¸° ë•Œë¬¸ì— ë¬´ë¦¬í•˜ê²Œ ì›€ì§ì´ë©´ ì˜¤íˆë ¤ ì—­íš¨ê³¼ê°€ ë‚  ìˆ˜ë„ ìˆë‹¤.\n",
      "ìš´ë™ì„ í•  ë•ŒëŠ” ëª¸ì„ ë”°ëœ»í•˜ê²Œ í•˜ê³  ë•€ì€ ì˜ í¡ìˆ˜í•˜ë„ë¡ í•´ì•¼ í•œë‹¤.</d> ì§€ë‚œë‹¬ 30ì¼ ì˜¤í›„ ì„œìš¸ ì¢…ë¡œêµ¬ ì„¸ì¢…ë¡œ ì •ë¶€ì¤‘ì•™ì²­ì‚¬ ë³„ê´€. ì´ë‚™ì—° êµ­ë¬´ì´ë¦¬ê°€ ì£¼ì¬í•˜ë˜ êµ­ë¬´íšŒì˜ê°€ ì—´ë ¸ë‹¤.\n",
      "êµ­ë¬´ìœ„ì›ë“¤ì´ ëª¨ë‘ ì°¸ì„í•œ ê°€ìš´ë° ì—´ë¦° ì´ë‚  íšŒì˜ì—ì„œëŠ” â€˜ê²½ì œí˜ì‹  3ê°œë…„ ê³„íšâ€™ ë“± ì£¼ìš” êµ­ì • í˜„ì•ˆì— ëŒ€í•œ ë…¼ì˜ê°€ ì´ë¤„ì¡Œë‹¤.\n",
      "ê¹€ë™ì—°(ì‚¬ì§„) ê²½ì œë¶€\n"
     ]
    }
   ],
   "source": [
    "text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ”'\n",
    "input_ids = tokenizer.encode(text) #encode : text -> vocabulary\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=200,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "\n",
    "print(gen_ids)\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist()) # decode() : tokenizer ì™€ vocabularyë¥¼ ì´ìš©í•´ì„œ token idë¥¼ stringìœ¼ë¡œ ë³€í™˜\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì—¬ê¸°ë¶€í„° watermark í•œêµ­ì–´ ì ìš© ì‹œì‘!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "/home/undergrad/miniconda3/envs/tf-2.12.0/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1595: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "2024-03-30 00:06:16.419206: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-30 00:06:16.461014: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-30 00:06:17.097194: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜›ë‚  ì˜›ë‚  ì–´ëŠ ë§ˆì„ì— í¥ë¶€ì™€ ë†€ë¶€ í˜•ì œê°€ ì™ìì§€ê»„ ë– ë“¤ì–´ëŒ€ë©° \"ìš°ë¦¬ ì§‘ì—” ì™œ ì´ë ‡ê²Œ ë§ì€ ì‚¬ëŒë“¤ì´ ëª¨ì—¬ ì‚¬ëŠ” ê±°ì•¼?\" í•˜ê³  ë¬»ëŠ”ë‹¤.\n",
      "ê·¸ëŸ°ë° ê·¸ ë§ˆì„ ì‚¬ëŒë“¤ì€ ëª¨ë‘ ë‹¤ë“¤ ìê¸°ë„¤ ë™ë„¤ì— ì‚´ê³  ìˆëŠ” ì‚¬ëŒë“¤ì´ë¼ê³  í•œë‹¤.\n",
      "ì´ë ‡ê²Œ í•´ì„œ ìš°ë¦¬ ë§ˆì„ì€ 'í¥ë¶€ê°€ ì‚´ë˜ ê³³'ì´ë¼ëŠ” ëœ»ì˜ 'ê³ í–¥'ì´ ë˜ì—ˆë‹¤.\n",
      "ê·¸ë¦¬ê³  ì´ ê³ í–¥ì€ ë°”ë¡œ ì§€ê¸ˆì˜ ì„œìš¸ ì¢…ë¡œêµ¬ ìˆ­ì¸ë™ì´ë‹¤.\n",
      "ìˆ­ì¸ë™ì€ ì›ë˜ ì¢…ë¡œì—ì„œ ê°€ì¥ ì˜¤ë˜ëœ ì£¼íƒê°€ì˜€ë‹¤.\n",
      "1970ë…„ëŒ€ê¹Œì§€ë§Œ í•´ë„ ì´ê³³ì€ ì¬ê°œë°œë¡œ ì¸í•´ í—ë¦¬ê³  ë¹ˆì§‘ì´ ë§ì•„ì¡Œë‹¤.\n",
      "í•˜ì§€ë§Œ 1980ë…„ëŒ€ ë“¤ì–´ ë‹¤ì‹œ í™œê¸°ë¥¼ ë˜ì°¾ê¸° ì‹œì‘í–ˆë‹¤.\n",
      "ë‹¹ì‹œë§Œ í•˜ë”ë¼ë„ ì´ê³³ì—ëŠ” ë‚¡ì€ ê±´ë¬¼ë“¤ì´ ë§ì´ ë‚¨ì•„ ìˆì—ˆë‹¤.\n",
      "ê·¸ëŸ¬ë‚˜ 1990ë…„ëŒ€ ë“¤ì–´ì„œë¶€í„°ëŠ” ì˜ˆì „ì˜\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, PreTrainedTokenizerFast\n",
    "import torch\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "#import torch\n",
    "#from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>') \n",
    "model = AutoModelWithLMHead.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "\n",
    "text = \"\"\" ì˜›ë‚  ì˜›ë‚  ì–´ëŠ ë§ˆì„ì— í¥ë¶€ì™€ ë†€ë¶€ í˜•ì œê°€ \"\"\"\n",
    "input_ids = tokenizer.encode(text)\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True\n",
    "                        )\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'reduce_sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# ë¡œì§“ ì¡°ì • ê°’\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# ì›Œí„°ë§ˆí‚¹ì´ ì ìš©ëœ ë‹¤ìŒ í† í° ìƒì„±\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m watermarked_token \u001b[38;5;241m=\u001b[39m \u001b[43mwatermarking_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mì›Œí„°ë§ˆí¬ í¬í•¨ëœ ì˜ˆì¸¡ í† í° : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwatermarked_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mwatermarking_logits\u001b[0;34m(prompt, previous_token, gamma, delta)\u001b[0m\n\u001b[1;32m      8\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# ì´ì „ í† í°ì˜ í•´ì‹œë¡œ ë‚œìˆ˜ ìƒì„±ê¸° ì´ˆê¸°í™”\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#seed = int(hashlib.sha256(previous_token.encode()).hexdigest(), 16) % 2**32\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#np.random.seed(seed)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_sum\u001b[49m(input_ids) \u001b[38;5;241m%\u001b[39m logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-2.12.0/lib/python3.8/site-packages/torch/__init__.py:1938\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1935\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 1938\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'reduce_sum'"
     ]
    }
   ],
   "source": [
    "\n",
    "def watermarking_logits(prompt, previous_token, gamma, delta):\n",
    "    # í”„ë¡¬í”„íŠ¸ì™€ ì´ì „ í† í°ìœ¼ë¡œ ì…ë ¥ êµ¬ì„±\n",
    "    input_ids = tokenizer.encode(prompt + previous_token, return_tensors='pt')\n",
    "\n",
    "    # ë¡œì§“ ìƒì„±\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "    # ì´ì „ í† í°ì˜ í•´ì‹œë¡œ ë‚œìˆ˜ ìƒì„±ê¸° ì´ˆê¸°í™”\n",
    "    #seed = int(hashlib.sha256(previous_token.encode()).hexdigest(), 16) % 2**32\n",
    "    #np.random.seed(seed)\n",
    "    seed = int(torch.reduce_sum(input_ids) % logits.shape[-1])\n",
    "    print(f\"seed : {seed}\")\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # ì „ì²´ í† í° ì§‘í•©ì„ ê·¸ë¦° ë¦¬ìŠ¤íŠ¸ì™€ ë ˆë“œ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ë¶„\n",
    "    token_indices = np.arange(logits.size(1)) #ìƒì„±í•  ìˆ˜ ìˆëŠ” ëª¨ë“  í† í°\n",
    "    print(f\"total tokens list size : {len(token_indices)} \")\n",
    "    green_list_size = int(len(token_indices) * gamma)\n",
    "    print(f\"green token list size : {green_list_size}\")\n",
    "    green_list_indices = np.random.choice(token_indices, green_list_size, replace=False)\n",
    "    print(f\"green token list : {green_list_indices}\")\n",
    "    \n",
    "    # ê·¸ë¦° ë¦¬ìŠ¤íŠ¸ì— ì†í•˜ëŠ” í† í°ì˜ ë¡œì§“ì— Î´ ë”í•˜ê¸° => ìƒì„± ì´‰ì§„\n",
    "    for idx in green_list_indices:\n",
    "        logits[0, idx] += delta\n",
    "\n",
    "    # ìˆ˜ì •ëœ ë¡œì§“ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í† í° ì˜ˆì¸¡\n",
    "    next_token_id = torch.argmax(logits, dim=-1) #ë§ˆì°¬ê°€ì§€ë¡œ ë§ˆì§€ë§‰ ì°¨ì›, ì „ì²´ ë¡œì§“ì¤‘ì—ì„œ ê°€ì¥ í° ê°’\n",
    "    next_token = tokenizer.decode(next_token_id)\n",
    "    \n",
    "    return next_token\n",
    "\n",
    "# ì˜ˆì‹œ ì‹¤í–‰\n",
    "prompt = \"ì´ê²ƒì€ ì˜ˆì‹œ ë¬¸ì¥ì…ë‹ˆë‹¤. \"\n",
    "previous_token = \"ì˜ˆì‹œ\"\n",
    "gamma = 0.5  # ê·¸ë¦° ë¦¬ìŠ¤íŠ¸ ë¹„ìœ¨\n",
    "delta = 0.1  # ë¡œì§“ ì¡°ì • ê°’\n",
    "\n",
    "# ì›Œí„°ë§ˆí‚¹ì´ ì ìš©ëœ ë‹¤ìŒ í† í° ìƒì„±\n",
    "watermarked_token = watermarking_logits(prompt, previous_token, gamma, delta)\n",
    "print(f\"ì›Œí„°ë§ˆí¬ í¬í•¨ëœ ì˜ˆì¸¡ í† í° : {watermarked_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded previous_token : ê¹Œ?\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [12818 32527  1365 ... 13888 45783 42271]\n",
      "next token : \"\n",
      "\n",
      "decoded previous_token : \"\n",
      "\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42159 21111 44876 ... 50794  4268 21424]\n",
      "next token : \"\n",
      "decoded previous_token : \"\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [10692 14837 17766 ... 17030  7821  9039]\n",
      "next token : í¥\n",
      "decoded previous_token : í¥\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [48266 35904  4251 ... 26463 26110  5132]\n",
      "next token : ë¶€\n",
      "decoded previous_token : ë¶€\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [39611  2347 40860 ... 11600 24423  1536]\n",
      "next token : ì—ê²Œ\n",
      "decoded previous_token : ì—ê²Œ\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [49850 37290  1581 ... 45209    90 48108]\n",
      "next token : ë¬´ì—‡ì„\n",
      "decoded previous_token : ë¬´ì—‡ì„\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [26783  5268 13710 ... 35357 25920 44496]\n",
      "next token : í–ˆ\n",
      "decoded previous_token : í–ˆ\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42090 25360 10031 ... 37300 26570 27395]\n",
      "next token : ì§€\n",
      "decoded previous_token : ì§€\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [44166 25530 11705 ... 11309 32169 27507]\n",
      "next token : ?\n",
      "decoded previous_token : ?\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [31049 32714 23032 ... 19986 13097 27945]\n",
      "next token : \"\n",
      "\n",
      "decoded previous_token : \"\n",
      "\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42159 21111 44876 ... 50794  4268 21424]\n",
      "next token : \"\n",
      "decoded previous_token : \"\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [10692 14837 17766 ... 17030  7821  9039]\n",
      "next token : í¥\n",
      "decoded previous_token : í¥\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [48266 35904  4251 ... 26463 26110  5132]\n",
      "next token : ë¶€\n",
      "decoded previous_token : ë¶€\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [39611  2347 40860 ... 11600 24423  1536]\n",
      "next token : ì—ê²Œ\n",
      "decoded previous_token : ì—ê²Œ\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [49850 37290  1581 ... 45209    90 48108]\n",
      "next token : ë¬´ì—‡ì„\n",
      "decoded previous_token : ë¬´ì—‡ì„\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [26783  5268 13710 ... 35357 25920 44496]\n",
      "next token : í–ˆ\n",
      "decoded previous_token : í–ˆ\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42090 25360 10031 ... 37300 26570 27395]\n",
      "next token : ì§€\n",
      "decoded previous_token : ì§€\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [44166 25530 11705 ... 11309 32169 27507]\n",
      "next token : ?\n",
      "decoded previous_token : ?\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [31049 32714 23032 ... 19986 13097 27945]\n",
      "next token : \"\n",
      "\n",
      "decoded previous_token : \"\n",
      "\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42159 21111 44876 ... 50794  4268 21424]\n",
      "next token : \"\n",
      "decoded previous_token : \"\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [10692 14837 17766 ... 17030  7821  9039]\n",
      "next token : í¥\n",
      "decoded previous_token : í¥\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [48266 35904  4251 ... 26463 26110  5132]\n",
      "next token : ë¶€\n",
      "decoded previous_token : ë¶€\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [39611  2347 40860 ... 11600 24423  1536]\n",
      "next token : ì—ê²Œ\n",
      "decoded previous_token : ì—ê²Œ\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [49850 37290  1581 ... 45209    90 48108]\n",
      "next token : ë¬´\n",
      "decoded previous_token : ë¬´\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [30431 41558 40217 ... 41728 16103 14174]\n",
      "next token : ì—‡\n",
      "decoded previous_token : ì—‡\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [28633 21803 50719 ... 18090 10793 30586]\n",
      "next token : ì„\n",
      "decoded previous_token : ì„\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [25638  1471 12313 ... 11944   847 47478]\n",
      "next token : í–ˆ\n",
      "decoded previous_token : í–ˆ\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42090 25360 10031 ... 37300 26570 27395]\n",
      "next token : ì§€\n",
      "decoded previous_token : ì§€\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [44166 25530 11705 ... 11309 32169 27507]\n",
      "next token : ?\n",
      "decoded previous_token : ?\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [31049 32714 23032 ... 19986 13097 27945]\n",
      "next token : \"\n",
      "\n",
      "decoded previous_token : \"\n",
      "\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42159 21111 44876 ... 50794  4268 21424]\n",
      "next token : \"\n",
      "decoded previous_token : \"\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [10692 14837 17766 ... 17030  7821  9039]\n",
      "next token : í¥\n",
      "decoded previous_token : í¥\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [48266 35904  4251 ... 26463 26110  5132]\n",
      "next token : ë¶€\n",
      "decoded previous_token : ë¶€\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [39611  2347 40860 ... 11600 24423  1536]\n",
      "next token : ì—ê²Œ\n",
      "decoded previous_token : ì—ê²Œ\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [49850 37290  1581 ... 45209    90 48108]\n",
      "next token : ë¬´\n",
      "decoded previous_token : ë¬´\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [30431 41558 40217 ... 41728 16103 14174]\n",
      "next token : ì—‡\n",
      "decoded previous_token : ì—‡\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [28633 21803 50719 ... 18090 10793 30586]\n",
      "next token : ì„\n",
      "decoded previous_token : ì„\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [25638  1471 12313 ... 11944   847 47478]\n",
      "next token : í–ˆ\n",
      "decoded previous_token : í–ˆ\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42090 25360 10031 ... 37300 26570 27395]\n",
      "next token : ì§€\n",
      "decoded previous_token : ì§€\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [44166 25530 11705 ... 11309 32169 27507]\n",
      "next token : ?\n",
      "decoded previous_token : ?\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [31049 32714 23032 ... 19986 13097 27945]\n",
      "next token : \"\n",
      "\n",
      "decoded previous_token : \"\n",
      "\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42159 21111 44876 ... 50794  4268 21424]\n",
      "next token : \"\n",
      "decoded previous_token : \"\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [10692 14837 17766 ... 17030  7821  9039]\n",
      "next token : í¥\n",
      "decoded previous_token : í¥\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [48266 35904  4251 ... 26463 26110  5132]\n",
      "next token : ë¶€\n",
      "decoded previous_token : ë¶€\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [39611  2347 40860 ... 11600 24423  1536]\n",
      "next token : ì—ê²Œ\n",
      "decoded previous_token : ì—ê²Œ\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [49850 37290  1581 ... 45209    90 48108]\n",
      "next token : ë¬´\n",
      "decoded previous_token : ë¬´\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [30431 41558 40217 ... 41728 16103 14174]\n",
      "next token : ì—‡\n",
      "decoded previous_token : ì—‡\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [28633 21803 50719 ... 18090 10793 30586]\n",
      "next token : ì„\n",
      "decoded previous_token : ì„\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [25638  1471 12313 ... 11944   847 47478]\n",
      "next token : í–ˆ\n",
      "decoded previous_token : í–ˆ\n",
      "==================================================\n",
      "total tokens list size : 51200 \n",
      "green token list size : 25600\n",
      "green token list : [42090 25360 10031 ... 37300 26570 27395]\n",
      "next token : ì§€\n",
      "decoded previous_token : ì§€\n",
      "==================================================\n",
      "ì˜›ë‚  ì˜›ë‚  ì–´ëŠ ë§ˆì„ì— í¥ë¶€ì™€ ë†€ë¶€ í˜•ì œê°€ ì‚´ê³  ìˆì—ˆë‹¤. í¥ë¶€ëŠ” ë†€ë¶€ì—ê²Œ ë¬´ì—‡ì„ í–ˆì„ê¹Œ?\"\n",
      "\"í¥ë¶€ì—ê²Œë¬´ì—‡ì„í–ˆì§€?\"\n",
      "\"í¥ë¶€ì—ê²Œë¬´ì—‡ì„í–ˆì§€?\"\n",
      "\"í¥ë¶€ì—ê²Œë¬´ì—‡ì„í–ˆì§€?\"\n",
      "\"í¥ë¶€ì—ê²Œë¬´ì—‡ì„í–ˆì§€?\"\n",
      "\"í¥ë¶€ì—ê²Œë¬´ì—‡ì„í–ˆì§€\n"
     ]
    }
   ],
   "source": [
    "def generate_with_watermarking(model, tokenizer, prompt, gamma, delta, max_length):\n",
    "    generated_text = prompt\n",
    "    previous_token = tokenizer.encode(prompt, add_special_tokens=False)[-1:] #í† í°í™”, ê°€ì¥ ë§ˆì§€ë§‰ í† í° ì„ íƒ\n",
    "    previous_token = tokenizer.decode(previous_token)\n",
    "    print(f\"decoded previous_token : {previous_token}\")\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        next_token = watermarking_logits(generated_text, previous_token, gamma, delta)\n",
    "        print(f\"next token : {next_token}\")\n",
    "        generated_text += next_token\n",
    "        previous_token = next_token\n",
    "        print(f\"decoded previous_token : {previous_token}\")\n",
    "\n",
    "        print(\"==================================================\")\n",
    "        if next_token == tokenizer.eos_token:\n",
    "            break\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "prompt = \"ì˜›ë‚  ì˜›ë‚  ì–´ëŠ ë§ˆì„ì— í¥ë¶€ì™€ ë†€ë¶€ í˜•ì œê°€ ì‚´ê³  ìˆì—ˆë‹¤. í¥ë¶€ëŠ” ë†€ë¶€ì—ê²Œ ë¬´ì—‡ì„ í–ˆì„ê¹Œ?\"\n",
    "max_length = 50  # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜\n",
    "gamma = 0.5  # ê·¸ë¦° ë¦¬ìŠ¤íŠ¸ ë¹„ìœ¨\n",
    "delta = 0.1  # ë¡œì§“ ì¡°ì • ê°’\n",
    "\n",
    "# ì›Œí„°ë§ˆí¬ í¬í•¨ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "watermarked_text = generate_with_watermarking(model, tokenizer, prompt, gamma, delta, max_length)\n",
    "#collection_generated_text += watermarked_text\n",
    "print(watermarked_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
